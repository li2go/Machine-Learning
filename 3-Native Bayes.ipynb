{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 [贝叶斯定理](https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86):\n",
    "\n",
    "贝叶斯定理是关于随机事件A和B的条件概率的一则定理:\n",
    "\n",
    "${\\displaystyle P(A|B)={\\frac {P(A)\\times P(B|A)}{P(B)}}}$\n",
    "\n",
    "其中$P(A|B)$是指在事件B发生的情况下事件A发生的概率。\n",
    "\n",
    "在贝叶斯定理中,每个名词都有约定俗成的名称:\n",
    "\n",
    "- $P(A|B)$是已知B发生后A的条件概率,也由于得自B的取值而被称作A的后验概率.\n",
    "- $P(A)$是A的先验概率(或边缘概率),之所以称为\"先验\"是因为它不考虑任何B方面的因素.\n",
    "- $P(B|A)$是已知A发生后B的条件概率,也由于得自A的取值而被称作B的后验概率.\n",
    "- $P(B)$是B的先验概率或边缘概率.\n",
    "\n",
    "也就是说:\n",
    "\n",
    "后验概率 = (似然性(Likelihood)*先验概率)/标准化常量\n",
    "\n",
    "更一般化的情况,假设$\\{A_i\\}$是事件集合里的部分集合,对于任意的$A_i$,贝氏定理可用下式表示:\n",
    "\n",
    "$P(A_{i}|B)={\\frac {P(B|A_{i})\\,P(A_{i})}{\\sum _{j}P(B|A_{j})\\,P(A_{j})}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData():\n",
    "    \"\"\"\n",
    "    Load data set \n",
    "    \n",
    "    Return:\n",
    "    ------\n",
    "        data: DataFrame.\n",
    "        \n",
    "    Note:\n",
    "    ----\n",
    "        last column: labels\n",
    "    \"\"\"\n",
    "    datasets = np.array([[1,1,1,1,1,2,2,2,2,2,3,3,3,3,3],\n",
    "                  ['S','M','M','S','S','S','M','M','L','L','L','M','M','L','L'],\n",
    "                  [-1,-1,1,1,-1,-1,-1,1,1,1,1,1,1,1,-1]]).T\n",
    "    columns = np.array(['X1','X2','Y'])\n",
    "\n",
    "    data = pd.DataFrame(data=datasets,columns=columns)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1 X2   Y\n",
       "0   1  S  -1\n",
       "1   1  M  -1\n",
       "2   1  M   1\n",
       "3   1  S   1\n",
       "4   1  S  -1\n",
       "5   2  S  -1\n",
       "6   2  M  -1\n",
       "7   2  M   1\n",
       "8   2  L   1\n",
       "9   2  L   1\n",
       "10  3  L   1\n",
       "11  3  M   1\n",
       "12  3  M   1\n",
       "13  3  L   1\n",
       "14  3  L  -1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = LoadData()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayes(data,test_X,gamma,is_print=False):\n",
    "    \"\"\"\n",
    "    Build Naive Bayes\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        data: training set.\n",
    "        test_X: testing set.\n",
    "        gamma: Laplace smoothing\n",
    "        is_print: is print label category and pos posterior probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 获取labels\n",
    "    labels = data.iloc[:,-1]\n",
    "    # 获取labels的类别个数K\n",
    "    labels_ = np.array(list(set(labels)))\n",
    "    len_labels_ = labels_.shape[0]\n",
    "    \n",
    "    # 先验概率\n",
    "    pri_prob= np.zeros((len_labels_))\n",
    "    \n",
    "    #>>>print(pri_prob)\n",
    "    #>>>[0. 0.]\n",
    "    # 预测值保留数组\n",
    "    m,n = test_X.shape\n",
    "    cache_predict = np.zeros((m,len_labels_))\n",
    "    \n",
    "    # 计算先验概率\n",
    "    for i in range(len_labels_):\n",
    "        P_y = (labels[labels == labels_[i]].size + gamma) / (labels.size + len_labels_ * gamma)\n",
    "        pri_prob[i] = P_y\n",
    "    \n",
    "    # 计算条件概率\n",
    "    for i in range(m):\n",
    "        for j in range(len_labels_):\n",
    "            Conditional_Prob = 1\n",
    "            for k in range(n):\n",
    "        \n",
    "                data_label = data[labels==labels_[j]] # 该标签下的所有数据\n",
    "                future_k = data_label.iloc[:,k]  # test_X 的第k个特征列\n",
    "                molecule = data_label[future_k == test_X[i,k]].shape[0] + gamma # 分子部分\n",
    "                Sj = len(set(future_k))\n",
    "                denominator = data_label.shape[0] + (Sj * gamma) # 分母部分\n",
    "                \n",
    "                Conditional_Prob *= molecule /denominator # 计算条件概率\n",
    "                \n",
    "            Pos_proba = pri_prob[j] * Conditional_Prob # 计算后验概率\n",
    "            \n",
    "            cache_predict[i,j] += Pos_proba\n",
    "        \n",
    "    # if ture,print labels_ and predict probability array.\n",
    "    if is_print:\n",
    "        print(labels_)\n",
    "        print(cache_predict)\n",
    "        \n",
    "    best_predict_index = np.argmax(cache_predict,axis=1)\n",
    "    return labels_[best_predict_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-1' '1']\n",
      "[[0.06100218 0.03267974]]\n",
      "Predict label is:  ['-1']\n"
     ]
    }
   ],
   "source": [
    "test_x = np.array([['2','S']])\n",
    "predic_label = NaiveBayes(data=data,test_X=test_x,gamma=1,is_print=True)\n",
    "print('Predict label is: ',predic_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不带拉普拉斯平滑项的贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算的是先验概率\n",
    "def cal_pre_prob(data):\n",
    "    y = data.iloc[:,-1]\n",
    "    y_size = y.size\n",
    "    prob_y = {}\n",
    "    for i in y:\n",
    "        if i not in prob_y:\n",
    "            prob_y[i] = 1\n",
    "        else:\n",
    "            prob_y[i] += 1\n",
    "    for key,value in prob_y.items():\n",
    "        prob_y[key] = value / y.size\n",
    "    return prob_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-1': 0.4, '1': 0.6}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_pre_prob(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = ['2','S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算条件概率\n",
    "def clac_condition_prob(data,test_x):\n",
    "    columns_x = data.columns[:-1]\n",
    "    columns_len = len(columns_x)\n",
    "    all_size = data.index.size\n",
    "    labels = data.iloc[:,-1]\n",
    "    labels_ = np.array(list(set(labels)))\n",
    "    len_labels_ = labels_.shape[0]\n",
    "    cond_Proba = 1\n",
    "    for i in range(columns_len):\n",
    "        #看有几个2和几个S\n",
    "        \n",
    "        for j in range(len_labels_):\n",
    "            data_label = data[labels==labels_[j]]\n",
    "            denominator = data_label.shape[0] #分母的部分\n",
    "            cond_prob = (data[columns_x[i]] == test_x[i]).sum() / denominator\n",
    "        \n",
    "        cond_Proba *= cond_prob\n",
    "    return cond_Proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24691358024691357"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clac_condition_prob(data,test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x(data,test_x):\n",
    "    y = data.iloc[:,-1]\n",
    "    prob_y = cal_pre_prob(data)\n",
    "    unique_y = pd.unique(y)\n",
    "    predict_y = []\n",
    "    for i in unique_y:\n",
    "        pri_prob = prob_y[i]\n",
    "        \n",
    "        #>>>print(pri_prob)\n",
    "        #>>>7\n",
    "        #   10\n",
    "        \n",
    "        s_data = data[y == i]\n",
    "        cond_Proba = clac_condition_prob(s_data,test_x) #拆分后的计算条件概率\n",
    "        houyan = pri_prob * cond_Proba\n",
    "        predict_y.append([i,houyan])\n",
    "        \n",
    "    print(predict_y)\n",
    "    predict = sorted(predict_y,key = lambda z:z[1],reverse = True)[0][0]\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-1', 0.06666666666666667], ['1', 0.02222222222222222]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-1'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_x(data,test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带拉普拉斯平滑项的贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算先验概率，将最后一列计数包含拉普拉斯平滑\n",
    "def cal_pre_prob(data,gamma):\n",
    "    y = data.iloc[:,-1] #取出最后一列y的标签\n",
    "    K = len(pd.unique(y)) #K代表y的类别\n",
    "    all_y_size = y.size + (K * gamma)\n",
    "    prob_y = {}\n",
    "    for i in y:\n",
    "        if i not in prob_y:\n",
    "            prob_y[i] = 1\n",
    "        else:\n",
    "            prob_y[i] += 1 \n",
    "        \n",
    "    for key,value in prob_y.items():\n",
    "        prob_y[key] = (value + gamma) / all_y_size\n",
    "    return prob_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-1': 0.4117647058823529, '1': 0.5882352941176471}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_pre_prob(data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算条件概率\n",
    "def clac_condition_prob(data,test_x,gamma):\n",
    "    colums = data.columns[:-1] \n",
    "    \n",
    "    #>>> print(colums)\n",
    "    #>>> ['X1', 'X2']\n",
    "    \n",
    "    all_size = data.index.size #计数最后一列\n",
    "    \n",
    "    #>>> print(all_size)\n",
    "    #>>> 15\n",
    "    cond_Proba = 1 \n",
    "    for i in range(len(colums)): #循环几个特征 要用下面拆分后的数据算\n",
    "        Sj = len(pd.unique(data[colums[i]]))\n",
    "        cond_prob = (data[colums[i]] == test_x[i]).sum() + gamma / (all_size + Sj * gamma)\n",
    "        cond_Proba *= cond_prob\n",
    "    return cond_Proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data,test_x):\n",
    "    unique_y = pd.unique(data.iloc[:,-1]) #去重\n",
    "    #>>>print(unique_y)\n",
    "    #>>>['-1' '1']\n",
    "    prob_y = cal_pre_prob(data,1)\n",
    "    predict_y = []\n",
    "    for i in unique_y:\n",
    "        pri_prob = prob_y[i]\n",
    "        \n",
    "        #>>>print(pri_prob)\n",
    "        #>>>7\n",
    "        #   10\n",
    "        \n",
    "        s_data = data[data.iloc[:,-1] == i]\n",
    "        cond_Proba = clac_condition_prob(s_data,test_x,1) #拆分后的计算条件概率\n",
    "        houyan = pri_prob * cond_Proba\n",
    "        predict_y.append([i,houyan])\n",
    "        \n",
    "    print(predict_y)\n",
    "    predict = sorted(predict_y,key = lambda z:z[1],reverse = True)[0][0]\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-1', 2.7044299201161945], ['1', 1.9648692810457515]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-1'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data(data,test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### pip3 install -i https://pypi.douban.com/simple scikit-learn     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过sklearn 调用Native bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a,b = X_train,y_train\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(a,b)\n",
    "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "print(clf.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def create_data():\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['label'] = iris.target\n",
    "    df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']\n",
    "    data = np.array(df.iloc[:100, [0, 1, -1]])\n",
    "    return data[:,:2], data[:,-1]\n",
    "X, y = create_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  GaussionNB   sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "[1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array(X_train)\n",
    "Y = np.array(y_train)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "GaussianNB(priors=None, var_smoothing=1e-09)\n",
    "print(clf.predict(X_test))\n",
    "\n",
    "clf_pf = GaussianNB()\n",
    "clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "GaussianNB(priors=None, var_smoothing=1e-09)\n",
    "print(clf_pf.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComplementNB  sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X,y = X_train,y_train\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "clf = ComplementNB()\n",
    "clf.fit(X, y)\n",
    "ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)\n",
    "print(clf.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据集\n",
    "def loadDataSet():\n",
    "    \n",
    "    postingList = [['my','dog','has','flea','problems','help','please'],\n",
    "                  ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "                  ['my','dalmation','is','so','cute','I','love','him'],\n",
    "                  ['stop','posting','stupid','worthless','grabage'],\n",
    "                  ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "                  ['quit','buying','worthless','dog','food','stupid']]\n",
    "    classVec = np.array([0,1,0,1,0,1]) # 1 is absive,0 not\n",
    "    \n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       " ['stop', 'posting', 'stupid', 'worthless', 'grabage'],\n",
       " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       " ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postingList,classVec = loadDataSet()\n",
    "postingList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Vocabulary(postingList):\n",
    "    #第一步：去重\n",
    "    #这种去重效率低\n",
    "    #Voc_list = []\n",
    "    #for words in postingList:\n",
    "        #for word in words:\n",
    "            #if word not in Voc_list:\n",
    "                #Voc_list.append(word)\n",
    "    Voc_list = set([])\n",
    "    for words in postingList:\n",
    "        Voc_list = Voc_list | set(words)\n",
    "    #去重后的文本\n",
    "    return list(Voc_list)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary = create_Vocabulary(postingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2Vec(Vocabulary,postingList):\n",
    "    m,n = len(postingList),len(Vocabulary)\n",
    "    #转换成零向量\n",
    "    Words_Vec = np.zeros((m,n))\n",
    "    for i in range(m):\n",
    "        for word in postingList[i]:\n",
    "            try:\n",
    "                index = Vocabulary.index(word)\n",
    "                Words_Vec[i,index] = 1\n",
    "            except:\n",
    "                print('这个{}不在我们的词集中'.format(word))\n",
    "            \n",
    "    return Words_Vec        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words_Vec = words2Vec(Vocabulary,postingList)\n",
    "Words_Vec#不能保证位置一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(X,y,X_test,Vocabulary,gamma):\n",
    "    \n",
    "    \n",
    "    uniuqe_y = np.unique(y)\n",
    "    K = len(uniuqe_y)\n",
    "     \n",
    "    all_y_size = y.shape[0] + (K * gamma)\n",
    "    #all_y_size = 8\n",
    "    prob_b = {}\n",
    "    for i in y:\n",
    "        if i not in prob_b:\n",
    "            prob_b[i] = 1\n",
    "        else:\n",
    "            prob_b[i] += 1\n",
    "    for key,value in prob_b.items():\n",
    "        prob_b[key] = (value + gamma) / all_y_size \n",
    "    \n",
    "    #>>>print(prob_b)\n",
    "    #>>>{0: 0.5, 1: 0.5}\n",
    "    predict_ = []\n",
    "    # 计算先验概率\n",
    "    \n",
    "    #P_1 =( y.sum()+gamma) /(y.shape[0]+K*gamma)\n",
    "    #P_1 = 0.5\n",
    "    \n",
    "    #P_0 = 1 - P_1\n",
    "    #P_y = {0:P_0,1:P_1}\n",
    "    test_x = words2Vec(Vocabulary,X_test)\n",
    "    #print(P_1)\n",
    "    # 计算条件概率\n",
    "    for label in uniuqe_y:\n",
    "        Sj = np.unique(X[label]).shape[0]\n",
    "        pri_P_y = prob_b[label]\n",
    "        Index = np.where(y==label)[0]\n",
    "        split_X = X[Index]\n",
    "        #Sj = 2\n",
    "        cond_fenmu = split_X.shape[0] + (Sj * gamma)\n",
    "        \n",
    "        sum_feature = np.sum(test_x == split_X,axis=0) + gamma\n",
    "        cond_prob = np.product(sum_feature / cond_fenmu) \n",
    "        \n",
    "        houyan = pri_P_y * cond_prob\n",
    "        predict_.append([label,houyan])\n",
    "    print(predict_)\n",
    "    predict = sorted(predict_,key=lambda z:z[1],reverse=True)[0][0]\n",
    "    return predict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个SB不在我们的词集中\n",
      "[[0, 8.374976495501263e-07], [1, 1.9606712234552562e-07]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = [['my','SB']]\n",
    "Model(X=Words_Vec,y=classVec,X_test=X_test,Vocabulary=Vocabulary,gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classVec.sum() / classVec.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.where(classVec == 0)[0]\n",
    "index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
